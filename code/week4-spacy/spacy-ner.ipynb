{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "radical-radical",
   "metadata": {},
   "source": [
    "# Named Entity Recognition in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ordinary-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.utilities import print_pipeline_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-congo",
   "metadata": {},
   "source": [
    "## Using the standard model\n",
    "\n",
    "This is simply a matter of just loading a language model and running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attractive-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amended-saying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0  3  Sue\n",
      "13 19  London\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Sue lives in London.\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.start_char:2} {ent.end_char:2}  {ent.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-public",
   "metadata": {},
   "source": [
    "However, we want to approve on this and maybe add a few bells and whistles and spaCy gives us a few options here:\n",
    "\n",
    "1. Adding a rule-based component\n",
    "2. Adding a custom component that enriches the document\n",
    "3. Updating the model included in \"en_core_web_sm\"\n",
    "\n",
    "We look into the first two here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-vancouver",
   "metadata": {},
   "source": [
    "## Rule-based entity recognition\n",
    "\n",
    "For this we add a component to the pipeline. Let's first see what is in the nlp pipeline we have used so far. It's an instance of `spacy.lang.en.English` which has a tokenizer and a set of further components. Note how the tokenizer has a special status and how it is not part of the components list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "headed-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x111016100>\n",
      "<spacy.tokenizer.Tokenizer object at 0x121271670> \n",
      "\n",
      "tok2vec          <spacy.pipeline.tok2vec.Tok2Vec object at 0x121424c70>\n",
      "tagger           <spacy.pipeline.tagger.Tagger object at 0x12144b540>\n",
      "parser           <spacy.pipeline.dep_parser.DependencyParser object at 0x1212819a0>\n",
      "senter           <spacy.pipeline.senter.SentenceRecognizer object at 0x121469f90>\n",
      "ner              <spacy.pipeline.ner.EntityRecognizer object at 0x1212814c0>\n",
      "attribute_ruler  <spacy.pipeline.attributeruler.AttributeRuler object at 0x12140ecc0>\n",
      "lemmatizer       <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x1214dc240>\n"
     ]
    }
   ],
   "source": [
    "print(nlp)\n",
    "print(nlp.tokenizer, '\\n')\n",
    "for name, component in nlp.components:\n",
    "    print(f\"{name:16} {component}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "located-authentication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METADATA       lang name version spacy_version description author email url ...\n",
      "NAME           core_web_sm\n",
      "DESCRIPTION    English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ...\n",
      "PIPELINE       tok2vec tagger parser ner attribute_ruler lemmatizer\n",
      "COMPONENTS     tok2vec tagger parser senter ner attribute_ruler lemmatizer\n",
      "LABELS         tok2vec tagger parser senter ner attribute_ruler lemmatizer\n",
      "LABELS[\"ner\"]  CARDINAL DATE EVENT FAC GPE LANGUAGE LAW LOC MONEY NORP ...\n"
     ]
    }
   ],
   "source": [
    "print('METADATA       %s' % ' '.join(list(nlp.meta.keys())[:8]), '...')\n",
    "print('NAME           %s' % nlp.meta['name'])\n",
    "print('DESCRIPTION    %s' % nlp.meta['description'][:80], '...')\n",
    "print('PIPELINE       %s' % ' '.join(list(nlp.meta['pipeline'])))\n",
    "print('COMPONENTS     %s' % ' '.join(list(nlp.meta['components'])))\n",
    "print('LABELS         %s' % ' '.join(list(nlp.meta['labels'].keys())))\n",
    "print('LABELS[\"ner\"]  %s' % ' '.join(list(nlp.meta['labels']['ner'][:10])), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-brain",
   "metadata": {},
   "source": [
    "| Name          | Description         \n",
    "| :-            | :-------------\n",
    "| NLP           | <spacy.lang.en.English object at 0x10e8e5f10>\n",
    "| NAME          | core_web_sm\n",
    "| DESCRIPTION   | English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "| METADATA      | lang name version spacy_version description author email url license spacy_git_version vectors labels pipeline components disabled performance sources requirements\n",
    "| COMPONENTS    | tok2vec tagger parser senter ner attribute_ruler lemmatizer\n",
    "| LABELS        | tok2vec tagger parser senter ner attribute_ruler lemmatizer\n",
    "| LABELS[\"ner\"] | CARDINAL DATE EVENT FAC GPE LANGUAGE LAW LOC MONEY NORP ORDINAL ORG PERCENT PERSON PRODUCT QUANTITY TIME WORK_OF_ART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-jungle",
   "metadata": {},
   "source": [
    "### A very simple pipeline\n",
    "\n",
    "You can start a pipeline from scratch using `spacy.lang.en.English` or `spacy.blank(\"en\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "celtic-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-embassy",
   "metadata": {},
   "source": [
    "This pipeline has no components, no description and a default name, but it will have a tokenizer, which is the minimal requirement to turn a text into a document. It wil have the same type as the pipleline loaded from `en_core_web_sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "altered-fever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x1216dbe80>\n",
      "<spacy.tokenizer.Tokenizer object at 0x122371ca0>\n",
      "\n",
      "NAME           pipeline\n",
      "DESCRIPTION    \n",
      "PIPELINE       []\n",
      "COMPONENTS     []\n",
      "LABELS         []\n",
      "LABELS[\"ner\"]  []\n",
      "\n",
      "type(nlp) == type(nlp_e)  ==>  True\n"
     ]
    }
   ],
   "source": [
    "nlp_e = English()\n",
    "\n",
    "print_pipeline_info(nlp_e)\n",
    "\n",
    "print(\"\\ntype(nlp) == type(nlp_e)  ==> \", type(nlp) == type(nlp_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "center-ghana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x1216dbe80>\n",
      "<spacy.tokenizer.Tokenizer object at 0x122371ca0>\n",
      "\n",
      "NAME           pipeline\n",
      "DESCRIPTION    \n",
      "PIPELINE       []\n",
      "COMPONENTS     []\n",
      "LABELS         []\n",
      "LABELS[\"ner\"]  []\n"
     ]
    }
   ],
   "source": [
    "# this does the same\n",
    "\n",
    "nlp_e2 = spacy.blank(\"ne\")\n",
    "print_pipeline_info(nlp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developing-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "# We have no entities yet...\n",
    "\n",
    "nlp_e = English()\n",
    "doc_e = nlp_e(\"Apple is opening its first big office in San Francisco.\")\n",
    "print(doc_e.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-motion",
   "metadata": {},
   "source": [
    "Now let's put in a rule-based entity extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "honey-background",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x122664fd0>\n",
      "<spacy.tokenizer.Tokenizer object at 0x1214cd040>\n",
      "\n",
      "entity_ruler     <spacy.pipeline.entityruler.EntityRuler object at 0x12265b980>\n",
      "\n",
      "NAME           pipeline\n",
      "DESCRIPTION    \n",
      "PIPELINE       ['entity_ruler']\n",
      "COMPONENTS     ['entity_ruler']\n",
      "LABELS         ['entity_ruler']\n",
      "LABELS[\"ner\"]  []\n"
     ]
    }
   ],
   "source": [
    "nlp_e = English()\n",
    "\n",
    "# the names you can use here are fixed, you cannot call it \"special_entities\" for example\n",
    "ruler = nlp_e.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# you can use token-based patterns just as before, the first one matches to the text field by default\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "print_pipeline_info(nlp_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "returning-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'en',\n",
       " 'name': 'pipeline',\n",
       " 'version': '0.0.0',\n",
       " 'spacy_version': '>=3.0.3,<3.1.0',\n",
       " 'description': '',\n",
       " 'author': '',\n",
       " 'email': '',\n",
       " 'url': '',\n",
       " 'license': '',\n",
       " 'spacy_git_version': 'f4f46b617',\n",
       " 'vectors': {'width': 0, 'vectors': 0, 'keys': 0, 'name': None},\n",
       " 'labels': {'entity_ruler': ['GPE', 'ORG']},\n",
       " 'pipeline': ['entity_ruler'],\n",
       " 'components': ['entity_ruler'],\n",
       " 'disabled': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_e.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "impressive-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Span', 'Apple', 'ORG'), ('Span', 'San Francisco', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "doc_e = nlp_e(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.__class__.__name__, ent.text, ent.label_) for ent in doc_e.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-buyer",
   "metadata": {},
   "source": [
    "### Inserting the rule-based NER in a full pipeline\n",
    "\n",
    "You can insert a specialized matcher before the \"ner\" component in the pipeline. Say we want to recognize gadgets as a special kind of product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "split-behavior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x122664790>\n",
      "<spacy.tokenizer.Tokenizer object at 0x1214cd670>\n",
      "\n",
      "tok2vec          <spacy.pipeline.tok2vec.Tok2Vec object at 0x121424ef0>\n",
      "tagger           <spacy.pipeline.tagger.Tagger object at 0x1226d2400>\n",
      "parser           <spacy.pipeline.dep_parser.DependencyParser object at 0x1224c9c40>\n",
      "senter           <spacy.pipeline.senter.SentenceRecognizer object at 0x1226d2630>\n",
      "entity_ruler     <spacy.pipeline.entityruler.EntityRuler object at 0x121932440>\n",
      "ner              <spacy.pipeline.ner.EntityRecognizer object at 0x12265aee0>\n",
      "attribute_ruler  <spacy.pipeline.attributeruler.AttributeRuler object at 0x122720200>\n",
      "lemmatizer       <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x12272be40>\n",
      "\n",
      "NAME           core_web_sm\n",
      "DESCRIPTION    English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
      "PIPELINE       ['tok2vec', 'tagger', 'parser', 'entity_ruler', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "COMPONENTS     ['tok2vec', 'tagger', 'parser', 'senter', 'entity_ruler', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "LABELS         ['tok2vec', 'tagger', 'parser', 'senter', 'entity_ruler', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "LABELS[\"ner\"]  ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n"
     ]
    }
   ],
   "source": [
    "nlp_e2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ruler = nlp_e2.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [{\"label\": \"GADGET\", \"pattern\": \"Apple iPhone\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "print_pipeline_info(nlp_e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-luxembourg",
   "metadata": {},
   "source": [
    "With the updated pipeline we can now get GADGETS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interstate-registration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple iPhone', 'GADGET'), ('today', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "#nlp_e2.enable_pipe('entity_ruler')\n",
    "doc = nlp_e2(\"I got my Apple iPhone today.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-bunny",
   "metadata": {},
   "source": [
    "Compare this to running it without the extra component. Note how the \"ner\" module did not create the ORG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stupid-agriculture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE:    ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "COMPONENTS:  ['tok2vec', 'tagger', 'parser', 'senter', 'entity_ruler', 'ner', 'attribute_ruler', 'lemmatizer'] \n",
      "\n",
      "[('Apple', 'ORG'), ('today', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "nlp_e2.disable_pipe('entity_ruler')\n",
    "\n",
    "print('PIPELINE:   ', [name for name, component in nlp_e2.pipeline])\n",
    "print('COMPONENTS: ', nlp_e2.component_names, '\\n')\n",
    "\n",
    "doc = nlp_e2(\"I got my Apple iPhone today.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-offset",
   "metadata": {},
   "source": [
    "### Adding a post NER component\n",
    "\n",
    "You can define and register your own components by using the `Language.component` decorator on some function. The function should take an instance of Doc as input and return the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "moderate-television",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.count_entities(doc)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "nlp_e3 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# First custom component\n",
    "@Language.component('count-ner-before')\n",
    "def count_entities(doc):\n",
    "    print(f\"I found {len(doc.ents)} named entities\")\n",
    "    return doc\n",
    "\n",
    "# Another one, you can use the same embedded function, will in real life probably refactor this\n",
    "@Language.component('count-ner-after')\n",
    "def count_entities(doc):\n",
    "    print(f\"I found {len(doc.ents)} named entities\")\n",
    "    return doc\n",
    "\n",
    "nlp_e3.add_pipe('count-ner-before', before=\"ner\")\n",
    "nlp_e3.add_pipe('count-ner-after', after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "personalized-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x1224d5490>\n",
      "<spacy.tokenizer.Tokenizer object at 0x122708310>\n",
      "\n",
      "tok2vec          <spacy.pipeline.tok2vec.Tok2Vec object at 0x123730310>\n",
      "tagger           <spacy.pipeline.tagger.Tagger object at 0x123730810>\n",
      "parser           <spacy.pipeline.dep_parser.DependencyParser object at 0x1229878e0>\n",
      "senter           <spacy.pipeline.senter.SentenceRecognizer object at 0x12237ad60>\n",
      "count-ner-before <function count_entities at 0x122371820>\n",
      "ner              <spacy.pipeline.ner.EntityRecognizer object at 0x1224d44c0>\n",
      "count-ner-after  <function count_entities at 0x122708ee0>\n",
      "attribute_ruler  <spacy.pipeline.attributeruler.AttributeRuler object at 0x12378d500>\n",
      "lemmatizer       <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x123790580>\n",
      "\n",
      "NAME           core_web_sm\n",
      "DESCRIPTION    English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
      "PIPELINE       ['tok2vec', 'tagger', 'parser', 'count-ner-before', 'ner', 'count-ner-after', 'attribute_ruler', 'lemmatizer']\n",
      "COMPONENTS     ['tok2vec', 'tagger', 'parser', 'senter', 'count-ner-before', 'ner', 'count-ner-after', 'attribute_ruler', 'lemmatizer']\n",
      "LABELS         ['tok2vec', 'tagger', 'parser', 'senter', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "LABELS[\"ner\"]  ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n"
     ]
    }
   ],
   "source": [
    "print_pipeline_info(nlp_e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "surrounded-lotus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 0 named entities\n",
      "I found 2 named entities\n"
     ]
    }
   ],
   "source": [
    "doc_e3 = nlp_e3(\"Sue lives in London.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-record",
   "metadata": {},
   "source": [
    "### Changing the Doc\n",
    "\n",
    "You can change the doc, for example by resetting `doc.ents` to a new value if you want. \n",
    "\n",
    "But you can also set custom variables on Doc, Span and Token instances. These attributes are accessible via `Doc._.`, `Span._.` and `Token._.` and allow you to add information.\n",
    "\n",
    "```python\n",
    ">>> Token.set_extension(\"is_color\", default=False)\n",
    ">>> doc = nlp(\"The sky is blue.\")\n",
    ">>> doc[3]._.is_color = True\n",
    "```\n",
    "\n",
    "You typically do this in a custom component. Below is a somewhat trivial example, but the technique itself is very powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "loaded-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "apparent-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.count_entities(doc)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_e4 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# setting the extension, first removing it if it were already there.\n",
    "if Doc.has_extension(\"ner_count\"):\n",
    "    Doc.remove_extension(\"ner_count\")\n",
    "Doc.set_extension(\"ner_count\", default=0)\n",
    "\n",
    "# define the custom component as a function\n",
    "@Language.component('count-ner')\n",
    "def count_entities(doc):\n",
    "    doc._.ner_count = len(doc.ents)\n",
    "    return doc\n",
    "\n",
    "# adding it to the pipeline just after ner\n",
    "nlp_e4.add_pipe('count-ner', after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "worthy-present",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running the pipeline and accessing the result\n",
    "doc_e4 = nlp_e4(\"Sue lives in London.\")\n",
    "doc_e4._.ner_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
